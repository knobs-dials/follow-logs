#!/usr/bin/python3
import os, sys
import time, datetime
import threading, signal, subprocess
import json, argparse, re, platform

try:
    import helpers_shellcolor as sc
except ImportError:
    sc = None


#### helpers ###########################################################

def parse_hms(v):
    ''' string interval spec to seconds, for times at most a bunch of days, e.g.  
            '1h30m' -> 5400, 
            '1 day, 30 min, 4 sec' -> 88204
        Very naive implementation (just looks for words starting with w,d,h,m,s)
    '''
    ret_sec = 0
    things = re.findall(r'([0-9.]+)\s*([a-z]+)[,;\s-]*', v.lower())
    if len(things)==0:
        try:
            ret_sec = float(v) # see if it's unitless, interpret as seconds
        except:
            raise ValueError("don't understand %r"%things)
    for num,unit in things:
        n = float(num)
        if unit[0]=='w':
            n *= 60*60*24*7
        elif unit[0]=='d':
            n *= 60*60*24
        elif unit[0]=='h':
            n *= 60*60
        elif unit[0]=='m':
            n *= 60
        elif unit[0]=='s':
            pass
        else:
            raise ValueError("don't understand time unit %r"%unit)
        ret_sec += n
    return ret_sec


def test_substr(test_str, string_list):
    ' Returns whether one of the strings in string_list is a substring of test_str '
    if string_list!=None and len(string_list)>0:
        for filtstr in string_list:
            if filtstr in test_str:
                return True
    return False


def path_indicates_possible_log(fullpath):
    """ Filename-based guess of whether this is likely to be a log file.
        Used to clean up entries from homedirs and other places where you may see a lot of non-log files
        Returns True for things we want to check, 
                False for things we know won't be logs (which will never be a complete list, but helps speed a little).
        In practice this does very little.         
    """
    basename = os.path.basename(fullpath)
    if fullpath.endswith('.gz') or fullpath.endswith('.bz2'): 
        #print( "filename NO %r"%fullpath )
        return False
    if ('atop_' in fullpath or basename.endswith('.jounal') or basename in ('btmp','wtmp')):
        #print( "filename NO %r"%fullpath )
        return False
    if (fullpath.endswith('.1') or fullpath.endswith('.2') or fullpath.endswith('.3') or
        fullpath.endswith('.4') or fullpath.endswith('.5') or fullpath.endswith('.6') or
        fullpath.endswith('.7') or fullpath.endswith('.8') or fullpath.endswith('.9')
        ): # assume these are rotated - so this is actually more of a 'not recent'
        #print( "filename NO %r"%fullpath )
        return False

    if (fullpath.endswith('.rb')  or fullpath.endswith('.gemspec') or fullpath.endswith('.conf') or
        fullpath.endswith('.cpp') or fullpath.endswith('.py')      or
        fullpath.endswith('.svn-base') or fullpath.endswith('.html')    or
        fullpath.endswith('.gif') or fullpath.endswith('.jpg')     or fullpath.endswith('.png')
        ):
        #print( "filename NO %r"%fullpath )
        return False
    if fullpath.endswith('.OU') or fullpath.endswith('.ER'):  # Torque PBS's convention
        #print( "filename MAYBE %r"%fullpath )
        return True
    if re.compile(r'\blog').search( os.path.basename(fullpath) ): # could be a little less wide, perhaps?
        #print( "filename MAYBE %r"%fullpath )
        return True
    #print( "filename UNDECIDED,MAYBE %r"%fullpath )
    return True


def file_is_recent(filename, recency_minutes=60):
    ''' Returns true if the youngest of (mtime,ctime) for the file is no longer than some amount of minutes ago
        ...compared to the system clock, so take care e.g. on embedded systems without an RTC.
    '''
    stob = os.stat(filename)
    latest_time = max([stob.st_mtime,stob.st_ctime])
    tdiff_sec = int(time.time()-latest_time)
    #print( filename )
    #print( '  latest change: %s'%latest_time )
    #print( '  time ago:      %s hours '%round(tdiff_sec/3600., 1) )
    if tdiff_sec < (60.*recency_minutes):
        return True
    return False


def file_is_nonbinary(filename=None, data=None, byte_amt=200):
    """ Given either a filename or some data from it, returns whether this is probably text, rather than binary data.
        amt: controls how much of data we use, or how much to read from the filename to be opened

        Written for 'is this possibly a log' test, so specialcases compressed files because of log rotation.
    """
    if data==None:
        if filename==None:
            raise ValueError("You need to give either a filename or some of the file's content data")
        else:
            f=open(filename,'rb')
            data=f.read(byte_amt)
            f.close()

    if data[:2]==b'\x1f\x8b': # gzip magic
        #print( 'Ignoring %r, is Gzip'%filename )
        return False
    elif data[:3]==b'BZh':    # bzip2 magic.  There's also 'BZ0' for bzip1, but meh.
        #print( "Ignoring %r, is bzip2"%filename )
        return False
    elif data[:3]==b'\xFD\x37\x7A\x58\x5A\x00':    # xz magic (TODO: test)
        #print( "Ignoring %r, is xz"%filename )
        return False

    printable,nonprintable=0,0
    for o in data[:byte_amt]:
        if (o>=0x20 and o<0x7f) or o in (0x0d, 0x0a):    # CONSIDER UTF-8?
            printable    += 1
        else:
            nonprintable += 1

    #print(filename, printable, nonprintable )
    if nonprintable==0: #only printable - this is text.
        return True
    else:
        # could be made slightly cleverer, but this is probably enough.
        printable_ratio = float(printable)/float(nonprintable+printable)
        if printable_ratio>0.95: # assume that's enough
            return True
        else:
            return False


    
class TailFollower(object):
    """ Tries to imitate tail -F as best we can, for the set of filenames that we are asked to follow.
    
        Written because actual tail -F seems to give up in some cases, which we cannot control.

        Opens filenames, keeps track of 
        - the file objects - so we can easily emit new lines
        - inode - so we can notice the file has changed  (log rotation or otherwise being overwritten)

        Could stand some further testing.
    """
    def __init__(self, print_callback, line_parser=None, source_format=None):
        ''' print_callback should take two strings, source and message 

            if line_parser !=None, we print line_parser(message)
                           ==None, we print message
        '''
        self.data = {} # filename -> ( fob, inode )
        self.print_callback = print_callback
        self.line_parser = line_parser
        self.source_format = source_format
        
    def add_filename(self, filename):
        ' start following this filename '
        if not os.path.exists(filename):
            raise OSError('File does not exist')
        if filename not in self.data:
            ino = os.stat(filename).st_ino
            fob = open(filename)
            fob.seek(0,2)        
            self.data[filename] = (fob, ino)

    def remove_filename(self, filename):
        ' stop following a filename. Not currently used, actually. '
        if filename in self.data:
            fob, ino = self.data[filename]
            fob.close()
            del self.data[filename]
        
    def per_file_work(self):
        ''' Does all the necessary work on the set of files we are currently following.
            You probably want to call this on a regular, shortish (<1sec) interval.
        
            One cycle of 
            - checking whether path disappeared - 
                if so, close and keep checking if something reappears for the same pat later
            - check whether file has been replaced - different inode from before
                if so, close old and open new
            - check if file shrunk
                if so, seek to end. Not 100% sure this is necessary for the OS but it helps consistency in our own seek logic
            - emitting lines from file we have open - based on when our seek position no longer matches the file's size
                calls the callback, which in this script is the printer function
       
            TODO: separate the file stat()s from the "are there new lines" checks
               ...though it's mostly stats on what is usually one or two dozen files, so not overly pricy
          '''
        # should deal with
        # - filename disappearing, and later reappearing (os.path.exists False somewhere. inode is not relevant)
        # - file being replaced   (different inode; should close and open)
        # ...largely because logrotate may cause both, depending on its config and how fast it is
        
        for filename in sorted(self.data.keys()):
            fob, ino = self.data[filename]

            if fob==None: # disappeared before, see if it reapeared
                if os.path.exists(filename):
                    print( "file reappeared, opening: %r"%filename )
                    fob = open(filename)
                    fob.seek(0,2)
                    ino = os.stat(filename).st_ino
                    self.data[filename] = (fob, ino)
                # CONSIDER: a (longish, configurable)  timeout to retry opens in
                
            else: # not marked disappeared
                try:
                    if not os.path.exists(filename):
                        print( "file disappeared, closing: %r"%filename )
                        fob.close()
                        self.data[filename] = (None, None)

                    else: # filename still exists, check for being replaecd (e.g. by logrotate)
                        stob = os.stat(filename)
                        if stob.st_ino != ino:
                            print( "file was replaced, reopening: %r"%filename )
                            fob.close()
                            fob = open(filename)
                            fob.seek(0,2)
                            self.data[filename] = (fob, stob.st_ino)

                    ### see if there are new lines to print
                    pos = fob.tell()
                    if pos != stob.st_size: # we assume things are only ever appended
                        if stob.st_size < pos: # ...mostly.
                            print( "file shrunk, seeking to end: %r"%filename )
                            fob.seek( 0,2 )
                        
                        for line in fob.readlines():
                            if self.line_parser:
                                line = self.line_parser( line )
                            source = os.path.basename(filename)
                            if self.source_format:
                                #print("source_format(%s)"%source)
                                source = self.source_format( source )
                            self.print_callback( source , line )

                # TODO: figure out all possible exceptions and for these cases more specifically, right now this is probably a little too catch-all

                # perhaps the most important if the file disappears _after_ the exists() check, things like the tell() and read() will fail instead, 
                except ValueError as ve:  # ...which seems to be reported as a ValueError
                    pass # we can ignore that because we'll get to it later
                
                except OSError as oe: # I got "telling position disabled by next() call", which seems to be our own thread's reading temporarily disabling tell (?)
                    pass # if so we can ignore that, we can print these lines next iteration


class TailFollower_Rescanner(object):
    ''' Represents a bunch of "how to scan for log files and where" state, and pokes the results into a given TailFollower object.
        Separate from TailFollower because you might have less entangled use for that class.
    '''
    def __init__(self, tailfollower, scan_dirs, onlys=None, nots=None, recency_minutes=60, add_home=False, verbose=False):
        ''' add_home         - look in current user's home directory
            recency_minutes  - log needs to be this young to be listed.   0/False/None means don't filter. 
        '''
        self.tailfollower    = tailfollower
        self.scan_dirs       = scan_dirs
        self.onlys           = onlys
        self.nots            = nots
        self.verbose         = verbose
        self.add_home        = add_home
        self.recency_minutes = recency_minutes
        #
        self.log_filenames = []

    def rescan(self):
        ''' Returns (how many logs, whether we changed this round)
            Returns list of log filenames from filesystem (via os.walk()s) with the configuration as listed in the object.
        '''       
        justnow_filenames = []
        
        if self.add_home: # TODO: we may want more restrictions here, e.g. looking only for filenames ending in .log
            scan_dirs.append( os.path.expanduser('~') )
        
        sawfiles_num = 0
        nopermission_num = 0
        for scan_dir in self.scan_dirs:
            try:
                if not os.path.exists(scan_dir):
                    continue
                # this indexing also does most filtering
                for curdir, _, files in os.walk( os.path.abspath(scan_dir) ):
                    for filename in files:
                        fullpath = os.path.join(curdir,filename)
                        sawfiles_num += 1

                        if self.onlys!=None and len(self.onlys)>0: # onlys was specified, filter according to it
                            if not test_substr( fullpath, self.onlys):
                                if self.verbose>=2:
                                    print( "SKIP: %60r  Doesn't match onlys (%r)"%(fullpath,self.onlys) )
                                continue

                        if self.nots!=None and len(self.nots)>0: # nots was specified, filter according to it
                            if test_substr( fullpath, self.nots):
                                if self.verbose>=2:
                                    print( "SKIP: %60r  Matches nots (%r)"%(fullpath,self.nots) )
                                continue
                            
                        if path_indicates_possible_log( fullpath ) == False:
                            if self.verbose >= 2:
                                print( "%60r  Filename doesn't look like log"%fullpath )
                            continue
                            
                        # above the content guess, below the filename based guess, because of relative cost
                        # (though mtime test should be almost free because os.walk stats too)
                        if self.recency_minutes:
                            if not file_is_recent(fullpath, recency_minutes=self.recency_minutes):
                                if self.verbose >= 2:
                                    print( "%60r  Not recent"%fullpath )
                                continue

                        try:
                            likely_log = file_is_nonbinary(fullpath, byte_amt=400)
                            if not likely_log:
                                if self.verbose >= 2:
                                    print( "%60r  Contents do not look like log"%fullpath )
                        except PermissionError: # quick hack to make it work, will address properly later
                            nopermission_num += 1

                        if self.verbose >= 2:
                            print( "%60r  Looks good"%fullpath )
                        justnow_filenames.append(fullpath)

                #if self.verbose:
                #    print("scanned %d files for logness"%sawfiles_num)
                #    if nopermission_num>0:
                #        print("no access to %d files"%nopermission_num)
                # TODO: give 'no permission to some files' error *once*
                                
            except Exception as exception:
                #if self.verbose:
                sys.stderr.write( "Error (%s), ignoring..."%str(exception))
                raise

        if self.verbose >= 2:
            print( "\nDone, found %d logs (in %d files)"%( len(justnow_filenames), sawfiles_num ) )

        # Check whether we saw new filenames
        #   we only care about seeing files we had not seen before,
        #   as tail deals with and mentions disappearance, and now-silent logs are not shown.
        new_files = set(justnow_filenames).difference( self.log_filenames )
        self.log_filenames = justnow_filenames
        if len(new_files) > 0:
            #if self.verbose >= 1:
            print("New file logs, now following %d"%len(self.log_filenames))
            #if self.verbose >= 2:
            #    print( self.log_filenames )
            for filename in new_files:
                #print( filename )
                self.tailfollower.add_filename( filename )



class DockerLogFollower(TailFollower):
    ''' Is 90% just TailFollower, because the only thing that really differs is 
        - how to parse lines (JSONL style)
        - how to notice new files
        - how to figure out the source name to show
    '''
    def __init__(self, print_callback):
        ''' print_callback should take two strings, source and message '''

        def line_parser(line):
            try:
                jd = json.loads( line )
                line = '[%s] %s: %s'%(jd['time'], jd['stream'], jd['log'])
            except:
                print( "Failed to parse %r as JSON"%line )
            return line

        def docker_source_format(source):
            source=source[:64] 
            # hackish cooperation with the rescanner code
            if source in self.containerid_name: 
                source = self.containerid_name[source].lstrip('/')
            return '{%s}'%source

        TailFollower.__init__(self, print_callback=print_callback, line_parser=line_parser, source_format=docker_source_format)

        self.containerid_name = {}


class DockerLogFollower_Rescanner(object):
    def __init__(self, dockerfollower, docker_dirs, onlys=None, nots=None, recency_minutes=None, verbose=False):
        self.dockerfollower  = dockerfollower
        self.docker_dirs     = docker_dirs
        self.onlys           = onlys
        self.nots            = nots
        self.verbose         = verbose
        self.recency_minutes = recency_minutes
        #
        self.docker_log_filenames = []


    def rescan(self):
        justnow_filenames = []

        for look_under in self.docker_dirs:
            if os.path.exists(look_under): # and test for dir?
                for container_id in os.listdir( look_under ):
                    if len(container_id) != 64:
                        continue
                    
                    log_filename = os.path.join(look_under, container_id, '%s-json.log'%container_id )

                    if self.recency_minutes:
                        if not file_is_recent(log_filename, recency_minutes=self.recency_minutes):
                            #if self.verbose >= 2:
                            print( "%60r  Not recent"%log_filename )
                            continue

                    container_name = container_id[:12]
                    with open(os.path.join(look_under, container_id, 'config.v2.json') ) as file:
                        docker_config = json.loads( file.read() )
                        if 'Name' in docker_config:
                            container_name = docker_config['Name']
                            image_name = docker_config['Config']['Image']

                            test_name = container_name + ' ' + image_name
                            if self.onlys!=None and len(self.onlys)>0: # onlys was specified, filter according to it
                                if not test_substr( test_name, self.onlys):
                                    if self.verbose>=2:
                                        print( "SKIP: %60r  Doesn't match onlys (%r)"%(test_name,self.onlys) )
                                    continue

                            if self.nots!=None and len(self.nots)>0: # nots was specified, filter according to it
                                if test_substr( test_name, self.nots):
                                    if self.verbose>=2:
                                        print( "SKIP: %60r  Matches nots (%r)"%(test_name,self.nots) )
                                    continue


                            self.dockerfollower.containerid_name[container_id] = container_name # hackish

                    justnow_filenames.append( log_filename )

        new_files = set(justnow_filenames).difference( self.docker_log_filenames )
        self.docker_log_filenames = justnow_filenames
        if len(new_files) > 0:
            #if self.verbose >= 1:
            print("New docker logs, now following %d"%len(self.docker_log_filenames))
            #if self.verbose >= 2:
            #print( self.docker_log_filenames )
            for filename in new_files:
                #print(filename)
                self.dockerfollower.add_filename( filename )


    
class JournalctlReader(object):
    """ Listening and printing for journalctl.

        runs journalctl on given units.
    """
    def __init__(self, print_callback, show_time=False):
        ''' print_callback should take two strings, source and message '''
        self.show_time = show_time
        self.pid = None
        self.proc = None
        self.print_callback = print_callback
        # TODO: take recency and turn into   --since "2 hours ago" style thing
        
    def change_units(self, units):
        ' start listening to new given set of units ' 
        if self.pid:
            os.kill( self.pid, signal.SIGKILL )
            self.proc.wait()
        cmd = [b'/bin/journalctl', b'-o',b'json', b'-f' ]
        for unit in units:
            cmd.extend( ['-u', unit] )
        self.proc = subprocess.Popen(cmd, shell=False, stdout=subprocess.PIPE)
        
    def show_new_messages(self):
        ''' One cycle of dealing with new messages.
            You probably want to call this on a regular, short (<1sec) interval.        
            ...but note that this blocks, which means you may want it in a separate thread
        '''
        if self.proc != None:
            line = self.proc.stdout.readline()   # becaues this blocks, the loop delay in thread can be tiny
            try:
                d = json.loads(line)

                ### I'm still figuring out which fields are present when. Documentation is barely helpful here...
                # these are spammy
                for debug_remove in ('__CURSOR', '_BOOT_ID', '_MACHINE_ID', '_HOSTNAME',
                                    # '__REALTIME_TIMESTAMP', '__MONOTONIC_TIMESTAMP', '_SOURCE_MONOTONIC_TIMESTAMP', 
                                    ):
                    if debug_remove in d:
                        del d[debug_remove]

                msg  = d.get('MESSAGE')

                if msg==None: # ...why?
                    print( "MESSAGE missing in %r"%d ) # leaving this in to see what cases this is
                    return # I think we can ignore this, though?
                

                transport = d.get('_TRANSPORT',None)
                if transport=='journal':
                    #print( d ) # figuring out the fields
                    sourcename = d.get('UNIT', 'unknown unit')        #sourcename = d.get('_SYSTEMD_UNIT',None) # this seems to be a scope.  Um, what?
                    if sourcename.endswith('.service'):
                        sourcename = sourcename[:-8]
                        
                elif transport=='kernel':
                    #print( d ) # figuring out the fields
                    t = d.get('_KERNEL_SUBSYSTEM') # not always present
                    if t==None:
                        t = d.get('SYSLOG_IDENTIFIER') # may be 'kernel'
                    if t=='kernel':    
                        sourcename = 'kernel'
                    else:
                        sourcename = 'kernel/%s'%t

                elif transport=='syslog':
                    sourcename = 'syslog/%s'%d.get('SYSLOG_IDENTIFIER')

                elif transport=='stdout': # service's stdout, try to figure uot service name
                    #print( d ) # figuring out the fields
                    t = d.get('_SYSTEMD_UNIT') # not always present
                    if t==None:
                        t = d.get('SYSLOG_IDENTIFIER') # may be 'kernel'
                    if t!=None:
                        if t.endswith('.service'):
                            t = t[:-8]
                        sourcename = '%s'%t
                    else:
                        sourcename = 'stdout'
                    if type(msg) is type([]): # what the hell?
                        msg = bytes(msg).decode('u8') # decode u8 is a guess, maybe add fallback
                
                elif transport=='driver':
                    #print( d ) # figuring out the fields
                    sourcename = 'driver'
                    
                elif transport=='audit':
                    #print( d ) # figuring out the fields
                    sourcename = 'audit'
                    
                else: # right now no others exist (according to docs, at least).   Though might it be missing?
                    #print( d ) # figuring out the fields
                    sourcename = transport

                if self.show_time:
                    tve = d.get('_SOURCE_REALTIME_TIMESTAMP') # TODO: figure out which of the timestamps is most useful
                    if tve!=None: # fall back to printing the current time. Not ideal in terms of monotonicity, but more helpful than nothing. CONSIDER: Put on parameter?
                        msg = datetime.datetime.fromtimestamp( float(tve)/1000000. ).strftime('%b %-d %H:%M:%S') + '  ' + msg
                    #else:
                    #    print( d )
                self.print_callback( '[%s]'%sourcename, msg )

            except (ValueError, KeyError) as e:  # AttributeError    TODO: do more localized error checking, remove the need for this.
                print( e )
                pass


class JournalctlReader_Rescanner(object):

    def __init__(self, journalctlreader, onlys=None, nots=None, verbose=False):
        self.journalctlreader = journalctlreader
        self.onlys   = onlys
        self.nots    = nots
        self.verbose = verbose
        #
        self.log_units = []

        # look up absolute path of systemctl, also to determine whether there's a systemd at all
        proc = subprocess.Popen(['/usr/bin/which', 'systemctl'], stdout=subprocess.PIPE )
        out1, _ = proc.communicate()
        self._systemctl_path = out1.strip()
        self.is_systemd = (len(self._systemctl_path.strip()) > 0)


    def journalctl_list(self, striptype=False):
        """ Helper for rescan_systemdunits():  call journalctl list-units, filter unit names.   """
        #self._systemctl_path
        proc = subprocess.Popen(['systemctl', b'list-units', b'--type=service',
                                     b'--plain', b'--no-legend',  # easier to parse
                                     b'--all'                     # not just active
                                     ], stdout=subprocess.PIPE )
        out, _ = proc.communicate()
        ret = []
        for line in out.splitlines():
            line = line.rstrip().decode('utf-8')  # encode: subprocess gives bytes, we're about to filter by cmdline arguments which are unicode
            unit, load, active, sub, description = line.split(None, 4)
            typeless, _ = unit.rsplit('.',1)
            if striptype: # from returned unit names
                unit = typeless

            if self.onlys!=None and len(self.onlys)>0:
                whitelist_condition = False
                for filtstr in self.onlys:
                    if filtstr in typeless:
                        whitelist_condition = True
                        break
                if not whitelist_condition:
                    if self.verbose >= 2:
                        print( "%60r  Doesn't match whitelist"%unit )
                    continue

            if self.nots!=None and len(self.nots)>0:
                blacklist_condition = False
                for filtstr in self.nots:
                    if filtstr in typeless:
                        blacklist_condition=True
                        break
                if blacklist_condition:
                    if self.verbose >= 2:
                        print( "%60r  Matches blacklist"%unit )
                    continue
            
            ret.append(unit)            
        return ret


    def rescan(self):
        """ Fetch list of matching unit names via journalctl_list(). """
        if not self.is_systemd:
            return
        units_now = self.journalctl_list()
        new_units = set(units_now).difference( self.log_units )
        self.log_units = units_now
        if len(new_units)>0:
            #if self.verbose >= 1:
            print("New units, now following %d"%len(self.log_units))
            if self.verbose >= 2:
                print( "  %s"%( ', '.join( sorted([v.replace('.service','') for v in new_units]) ))  )
            self.journalctlreader.change_units( self.log_units )
    
    

if __name__=='__main__':
    try: 
        import setproctitle    # I like my tmux titles informative
        setproctitle.setproctitle( os.path.basename(sys.argv[0]) )
    except ImportError:
        pass

    quit = False
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument("--recency",                  action="store",                              default="1d",     help="Exclude logs with mtime/ctime older than this, defaults to 1d")
        parser.add_argument('-o',"--onlys",               action="store",                              default=None,     help="If specified, match only names with one of these substrings (comma separated)")
        parser.add_argument('-n',"--nots",                action="store",                              default=None,     help="if specified, exclude names with one of these substrings (comma separated). Will apply after --recency and --only, if specified.")
        parser.add_argument("-T", "--no-time",            action="store_false",  dest='show_time',     default=True,     help="By default we add the time field to journalctl lines. Specify this to turn that off, and try (no promises) to remove date-n-time from the start of messages")
        parser.add_argument("-S", "--strip-more",         action="store_true",   dest="strip_more",    default=True,     help="Try to strip more uninformative things from the message, like the hostname")
        parser.add_argument("-J", "--no-journalctl",      action="store_false",  dest="do_journalctl", default=True,     help="Don't watch systemd units")
        parser.add_argument("-F", "--no-files",           action="store_false",  dest="do_files",      default=True,     help="Don't watch for file logs")
        parser.add_argument("-D", "--no-docker",          action="store_false",  dest="do_docker",     default=True,     help="Don't watch for docker logs")
        parser.add_argument("--check-interval",           action="store",       default="10s",                           help="check for log source changes every this often, defaults to 10s")
        parser.add_argument("--scandirs",                 action="store",       default="/var/log/,/var/lib/log/",       help="paths to look for logs under, comma-separated, defaults to /var/log/,/var/lib/log/ ")
        #parser.add_argument("--home",                     action="store_true",  default=False,                           help="look for log-ish text files in homedir, defaults off since it's slow and may turn up crud.")
        parser.add_argument("-g", "--grep",               action="store",       dest="grep",           default=None,     help="only print messages containing substring")        
        parser.add_argument("-G", "--grep-out",           action="store",       dest="grepout",        default=None,     help="don't print messages containing substring")
        parser.add_argument("-C", "--no-color",           action="store_true",  dest="nocolor",        default=False,    help="don't use color escapes, even when context seems to support it.")        
        parser.add_argument("-t", "--true-color",         action="store_true",  dest="truecolor",      default=False,    help="use true-color escapes, for more colors when you know it's supported.")
        parser.add_argument('-v', "--verbose",            action="count",                              default=0,        help="debug verbosity (repeat for debug)")
        args = parser.parse_args()
        #
        check_interval = max(1, parse_hms( args.check_interval ))
        recency_minutes = parse_hms( args.recency ) / 60.
        scan_dirs       = args.scandirs.split(',')
        onlys, nots = [], []
        if args.onlys:
            onlys.extend( args.onlys.split(',') )
        if args.nots:
            nots.extend(  args.nots.split(',')  )
        if args.nocolor:
            sc = None

        ## Decide on the printer callback ##########################################
        def print_message(source, message):
            " Unify printing style. "
            if args.grep != None: # CONSIDER: interpret comma separated?
                if args.grep not in message:
                    return
            elif args.grepout != None:
                if args.grepout in message:
                    return                

            if not args.show_time:
                # Try to remove dates from the start of messages. 
                #   may remove too much if it were a generic matches, so I'm removing formats as I see them. 
                #   this will always under-remove (because of "works on my system" logic), and more so in other locales.
                #   It's better than nothing and I'd rather err on the side of not removing information
                # CONSIDER: better ways of doing this, e.g. compiling patterns based on locale settings

                # 'Dec 24 17:55:44'
                message = re.sub(r'^([A-Za-z]{3}\s+[0-9]{1,2}\s[0-9]{1,2}:[0-9][0-9]:[0-9][0-9]\s{0,1})','', message)
                # '2021/12/24 17:55:15' and '2021/12/24-17:55:15'
                message = re.sub(r'^([0-9][0-9][0-9][0-9]/[0-9][0-9]/[0-9][0-9][\s-][0-9]{1,2}:[0-9][0-9]:[0-9][0-9]\s{0,2})','', message)
                # '[2021-12-24  18:03:01]'
                message = re.sub(r'^(\[[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\s+[0-9]{1,2}:[0-9][0-9]:[0-9][0-9]\]\s{0,2})','', message)
                # '[Fri Dec 24 18:08:03.356792 2021]'
                message = re.sub(r'^(\[[A-Za-z]{3,4}\s[A-Za-z]{3,4}\s[0-9]{1,2}\s[0-9]{1,2}:[0-9][0-9]:[0-9][0-9](?:[.][0-9]+)\s[0-9][0-9][0-9][0-9]\]\s{0,2})','', message)
                # '2022-01-29T23:25:42.778+0100' ISO8601 and some technically-non-conforming variants
                message = re.sub(r'^([\[]?[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]([.][0-9]{1,9})?([+-][0-9][0-9]:?[0-9][0-9]|Z)?[\]]?[\s]?)','', message)

            if args.strip_more:
                hostname_sp = platform.node()+' '
                # strip hostname from start of message (consider it being followed by space, :)
                if message.startswith( hostname_sp ): 
                    message = message[ len(hostname_sp): ].strip()

            align_source = '%25s'%source
            if sc==None: # we specified --no-color, or that color helper wasn't there
                sys.stdout.write( align_source )
            else:
                if args.truecolor:
                    # TODO: we can currently pick near-block colors that don't show up when converted to 8-color by the likes of tmux. avoid that.
                    sys.stdout.write( sc.hash_color( align_source, rgb=True ) )
                else: # basic ~8 colors plus some bright variants (or none if the helper detected no color support)
                    color_funcs = [ sc.blue, sc.red, sc.green, sc.cyan, sc.magenta, sc.yellow, sc.brightblue, sc.brightred, sc.brightcyan, sc.brightmagenta, sc.brightyellow, sc.brightgreen ]
                    strhash = sum( ord(ch)  for ch in source )
                    sys.stdout.write( color_funcs[strhash%len(color_funcs)]( align_source ) )
            sys.stdout.write( ': ')
            sys.stdout.write( message.rstrip() )
            sys.stdout.write( '\n')
            sys.stdout.flush()
       

        ### Main ########################################
        # Note: it's slightly easier to still instantiate them but then never use them based on do_* variables         
        tf           = TailFollower(                print_message )
        tf_rescanner = TailFollower_Rescanner(      tf, scan_dirs=['/var/log/'], onlys=onlys, nots=nots, recency_minutes=recency_minutes, verbose=args.verbose )

        df           = DockerLogFollower(           print_message )
        df_rescanner = DockerLogFollower_Rescanner( df, docker_dirs=['/var/lib/docker/containers/'], onlys=onlys, nots=nots, recency_minutes=recency_minutes, verbose=args.verbose )
        
        jr           = JournalctlReader(            print_message, show_time=args.show_time )
        jr_rescanner = JournalctlReader_Rescanner(  jr, onlys=onlys, nots=nots, verbose=args.verbose )


        def background_rescanner_thread():
            while True:
                if args.do_journalctl:
                    if args.verbose >= 2:
                        print( "Rescanning journald units" )
                    jr_rescanner.rescan()
                                        
                if args.do_files:
                    if args.verbose >= 2:
                        print( "Rescanning file logs" )
                    tf_rescanner.rescan()

                if args.do_docker:
                    if args.verbose >= 2:
                        print( "Rescanning docker logs" )
                    df_rescanner.rescan()

                if len( tf_rescanner.log_filenames )==0 and len( df_rescanner.docker_log_filenames )==0 and len(jr_rescanner.log_units)==0:
                    print("NOTE: There are 0 logs that match, you probably have a very restrictive filter.")
                time.sleep( check_interval )

        bg = threading.Thread( target=background_rescanner_thread )
        bg.daemon = True
        print( "Looking for logs..." )
        bg.start()

        if args.do_journalctl:
            def journalctl_reader_thread():
                while not quit:
                    jr.show_new_messages()
                    time.sleep(0.5) # don't spin when while there's no units we're watching on
            jrt = threading.Thread( target=journalctl_reader_thread )
            jrt.daemon = True        
            jrt.start()

        while not quit:
            if args.do_files:
                tf.per_file_work()
            if args.do_docker:
                df.per_file_work()
            time.sleep(0.2)
        
    except KeyboardInterrupt:
        quit = True
